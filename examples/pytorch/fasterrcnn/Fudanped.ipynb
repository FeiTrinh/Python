{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5bf391c-3dd3-4d0e-bc2b-7cb18a7bb2e4",
   "metadata": {},
   "source": [
    "MuskRCNN基础训练教程  \n",
    "https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html#finetuning-from-a-pretrained-model  \n",
    "# 头文件导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfca11ab-4c73-40e4-841a-05f2b64ab038",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8c588e-de15-4286-ac9b-a5960d73bd1f",
   "metadata": {},
   "source": [
    "# 数据集建立 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b3a99c5-90ce-455a-88da-b9f8c3b79b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PennFudanDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, transforms):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        # load all image files, sorting them to\n",
    "        # ensure that they are aligned\n",
    "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"PNGImages\"))))\n",
    "        self.masks = list(sorted(os.listdir(os.path.join(root, \"PedMasks\"))))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # load images and masks\n",
    "        img_path = os.path.join(self.root, \"PNGImages\", self.imgs[idx])\n",
    "        mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        # note that we haven't converted the mask to RGB,\n",
    "        # because each color corresponds to a different instance\n",
    "        # with 0 being background\n",
    "        mask = Image.open(mask_path)\n",
    "        # convert the PIL Image into a numpy array\n",
    "        mask = np.array(mask)\n",
    "        # instances are encoded as different colors\n",
    "        obj_ids = np.unique(mask)\n",
    "        # first id is the background, so remove it\n",
    "        obj_ids = obj_ids[1:]\n",
    "\n",
    "        # split the color-encoded mask into a set\n",
    "        # of binary masks\n",
    "        masks = mask == obj_ids[:, None, None]\n",
    "\n",
    "        # get bounding box coordinates for each mask\n",
    "        num_objs = len(obj_ids)\n",
    "        boxes = []\n",
    "        for i in range(num_objs):\n",
    "            pos = np.where(masks[i])\n",
    "            xmin = np.min(pos[1])\n",
    "            xmax = np.max(pos[1])\n",
    "            ymin = np.min(pos[0])\n",
    "            ymax = np.max(pos[0])\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "        # convert everything into a torch.Tensor\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        # there is only one class\n",
    "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
    "        masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
    "\n",
    "        image_id = torch.tensor([idx])\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        # suppose all instances are not crowd\n",
    "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"masks\"] = masks\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c5baec-dd57-4ece-b56d-71d71041f6f9",
   "metadata": {},
   "source": [
    "# 载入预训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b717f10-09e4-4607-97b1-7d2e9a674cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a model pre-trained on COCO\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "# replace the classifier with a new one, that has\n",
    "# num_classes which is user-defined\n",
    "num_classes = 2  # 1 class (person) + background\n",
    "# get number of input features for the classifier\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "# replace the pre-trained head with a new one\n",
    "model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e263ff3-eb10-44e7-8341-6b262773950c",
   "metadata": {},
   "source": [
    "# 修饰模型更改骨干网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66871c16-d516-4de8-b0c5-4b770eefb135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a pre-trained model for classification and return\n",
    "# only the features\n",
    "backbone = torchvision.models.mobilenet_v2(pretrained=True).features\n",
    "# FasterRCNN needs to know the number of\n",
    "# output channels in a backbone. For mobilenet_v2, it's 1280\n",
    "# so we need to add it here\n",
    "backbone.out_channels = 1280\n",
    "\n",
    "# let's make the RPN generate 5 x 3 anchors per spatial\n",
    "# location, with 5 different sizes and 3 different aspect\n",
    "# ratios. We have a Tuple[Tuple[int]] because each feature\n",
    "# map could potentially have different sizes and\n",
    "# aspect ratios\n",
    "anchor_generator = torchvision.models.detection.rpn.AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n",
    "                                   aspect_ratios=((0.5, 1.0, 2.0),))\n",
    "\n",
    "# let's define what are the feature maps that we will\n",
    "# use to perform the region of interest cropping, as well as\n",
    "# the size of the crop after rescaling.\n",
    "# if your backbone returns a Tensor, featmap_names is expected to\n",
    "# be [0]. More generally, the backbone should return an\n",
    "# OrderedDict[Tensor], and in featmap_names you can choose which\n",
    "# feature maps to use.\n",
    "roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'],\n",
    "                                                output_size=7,\n",
    "                                                sampling_ratio=2)\n",
    "\n",
    "# put the pieces together inside a FasterRCNN model\n",
    "model = torchvision.models.detection.FasterRCNN(backbone,\n",
    "                   num_classes=2,\n",
    "                   rpn_anchor_generator=anchor_generator,\n",
    "                   box_roi_pool=roi_pooler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540bb43a-0182-48ae-941f-43d8de6fcb09",
   "metadata": {},
   "source": [
    "# 模型建立"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2300e0a9-1a9b-4b26-a198-4957fccf7f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_instance_segmentation(num_classes):\n",
    "    # load an instance segmentation model pre-trained on COCO\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "    # get number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    # now get the number of input features for the mask classifier\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    # and replace the mask predictor with a new one\n",
    "    model.roi_heads.mask_predictor = torchvision.models.detection.mask_rcnn.MaskRCNNPredictor(in_features_mask,\n",
    "                                                       hidden_layer,\n",
    "                                                       num_classes)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c1abd6-4e34-4608-b364-0af3b823be84",
   "metadata": {},
   "source": [
    "# 转换函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4b7a21b-2a23-4040-a46f-04f8f78c0769",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transforms as T\n",
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    transforms.append(T.ToTensor())\n",
    "    if train:\n",
    "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "    return T.Compose(transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9379053c-ce51-40ad-8a93-64bd6973dfc0",
   "metadata": {},
   "source": [
    "# 前向函数建立"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64a542bf-d3e6-4f99-9822-98554c4509f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33febae3-158b-4354-b101-2c3d0c68acbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\11426\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'boxes': tensor([], size=(0, 4), grad_fn=<StackBackward>), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([], grad_fn=<IndexBackward>)}, {'boxes': tensor([], size=(0, 4), grad_fn=<StackBackward>), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([], grad_fn=<IndexBackward>)}]\n"
     ]
    }
   ],
   "source": [
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "dataset = PennFudanDataset('../datasets/PennFudanPed', get_transform(train=True))\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    " dataset, batch_size=2, shuffle=True, num_workers=0,\n",
    " collate_fn=collate_fn)\n",
    "# For Training\n",
    "images,targets = next(iter(data_loader))\n",
    "images = list(image for image in images)\n",
    "targets = [{k: v for k, v in t.items()} for t in targets]\n",
    "output = model(images,targets)   # Returns losses and detections\n",
    "# For inference\n",
    "model.eval()\n",
    "x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\n",
    "predictions = model(x)           # Returns predictions\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98d0aec2-f374-490e-ab85-4e6c2d1310bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]  [  0/120]  eta: 0:05:54  lr: 0.000047  loss: 8.0519 (8.0519)  loss_classifier: 0.6849 (0.6849)  loss_box_reg: 0.5395 (0.5395)  loss_mask: 6.7719 (6.7719)  loss_objectness: 0.0520 (0.0520)  loss_rpn_box_reg: 0.0036 (0.0036)  time: 2.9567  data: 0.0429  max mem: 1171\n",
      "Epoch: [0]  [ 10/120]  eta: 0:03:09  lr: 0.000467  loss: 4.8235 (4.3703)  loss_classifier: 0.6127 (0.5552)  loss_box_reg: 0.3535 (0.4242)  loss_mask: 3.3774 (3.3461)  loss_objectness: 0.0296 (0.0353)  loss_rpn_box_reg: 0.0086 (0.0096)  time: 1.7197  data: 0.0869  max mem: 1650\n",
      "Epoch: [0]  [ 20/120]  eta: 0:02:39  lr: 0.000886  loss: 1.1713 (2.6595)  loss_classifier: 0.2537 (0.3705)  loss_box_reg: 0.2488 (0.3333)  loss_mask: 0.6197 (1.9208)  loss_objectness: 0.0188 (0.0279)  loss_rpn_box_reg: 0.0045 (0.0071)  time: 1.5258  data: 0.0875  max mem: 1650\n",
      "Epoch: [0]  [ 30/120]  eta: 0:02:20  lr: 0.001306  loss: 0.5203 (1.9549)  loss_classifier: 0.1063 (0.2815)  loss_box_reg: 0.1537 (0.2722)  loss_mask: 0.2463 (1.3713)  loss_objectness: 0.0156 (0.0239)  loss_rpn_box_reg: 0.0032 (0.0060)  time: 1.4778  data: 0.0876  max mem: 1650\n",
      "Epoch: [0]  [ 40/120]  eta: 0:02:06  lr: 0.001726  loss: 0.4159 (1.6007)  loss_classifier: 0.0826 (0.2358)  loss_box_reg: 0.1216 (0.2569)  loss_mask: 0.1942 (1.0815)  loss_objectness: 0.0121 (0.0207)  loss_rpn_box_reg: 0.0032 (0.0058)  time: 1.5563  data: 0.1047  max mem: 1669\n",
      "Epoch: [0]  [ 50/120]  eta: 0:01:49  lr: 0.002146  loss: 0.4021 (1.3661)  loss_classifier: 0.0550 (0.1994)  loss_box_reg: 0.1546 (0.2376)  loss_mask: 0.1606 (0.9055)  loss_objectness: 0.0046 (0.0180)  loss_rpn_box_reg: 0.0038 (0.0056)  time: 1.5574  data: 0.1083  max mem: 1695\n",
      "Epoch: [0]  [ 60/120]  eta: 0:01:32  lr: 0.002565  loss: 0.3264 (1.1986)  loss_classifier: 0.0453 (0.1733)  loss_box_reg: 0.1247 (0.2205)  loss_mask: 0.1523 (0.7841)  loss_objectness: 0.0020 (0.0153)  loss_rpn_box_reg: 0.0022 (0.0054)  time: 1.4795  data: 0.0928  max mem: 1695\n",
      "Epoch: [0]  [ 70/120]  eta: 0:01:17  lr: 0.002985  loss: 0.3796 (1.1070)  loss_classifier: 0.0448 (0.1608)  loss_box_reg: 0.1528 (0.2219)  loss_mask: 0.1788 (0.7039)  loss_objectness: 0.0020 (0.0144)  loss_rpn_box_reg: 0.0067 (0.0062)  time: 1.5532  data: 0.1038  max mem: 1695\n",
      "Epoch: [0]  [ 80/120]  eta: 0:01:02  lr: 0.003405  loss: 0.4213 (1.0211)  loss_classifier: 0.0448 (0.1481)  loss_box_reg: 0.1528 (0.2109)  loss_mask: 0.1950 (0.6427)  loss_objectness: 0.0026 (0.0132)  loss_rpn_box_reg: 0.0067 (0.0062)  time: 1.6037  data: 0.1194  max mem: 1695\n",
      "Epoch: [0]  [ 90/120]  eta: 0:00:46  lr: 0.003825  loss: 0.3730 (0.9481)  loss_classifier: 0.0481 (0.1378)  loss_box_reg: 0.1002 (0.2010)  loss_mask: 0.1582 (0.5911)  loss_objectness: 0.0011 (0.0120)  loss_rpn_box_reg: 0.0065 (0.0062)  time: 1.5565  data: 0.1019  max mem: 1695\n",
      "Epoch: [0]  [100/120]  eta: 0:00:31  lr: 0.004244  loss: 0.3658 (0.8931)  loss_classifier: 0.0531 (0.1305)  loss_box_reg: 0.1138 (0.1936)  loss_mask: 0.1597 (0.5518)  loss_objectness: 0.0009 (0.0110)  loss_rpn_box_reg: 0.0065 (0.0064)  time: 1.5546  data: 0.0955  max mem: 1699\n",
      "Epoch: [0]  [110/120]  eta: 0:00:15  lr: 0.004664  loss: 0.3658 (0.8470)  loss_classifier: 0.0484 (0.1235)  loss_box_reg: 0.1138 (0.1862)  loss_mask: 0.1806 (0.5204)  loss_objectness: 0.0009 (0.0102)  loss_rpn_box_reg: 0.0061 (0.0066)  time: 1.6208  data: 0.1103  max mem: 1699\n",
      "Epoch: [0]  [119/120]  eta: 0:00:01  lr: 0.005000  loss: 0.3268 (0.8084)  loss_classifier: 0.0438 (0.1182)  loss_box_reg: 0.0813 (0.1802)  loss_mask: 0.1799 (0.4938)  loss_objectness: 0.0009 (0.0096)  loss_rpn_box_reg: 0.0060 (0.0066)  time: 1.6341  data: 0.1060  max mem: 1699\n",
      "Epoch: [0] Total time: 0:03:08 (1.5691 s / it)\n",
      "creating index...\n",
      "index created!\n",
      "Test:  [ 0/50]  eta: 0:00:33  model_time: 0.6408 (0.6408)  evaluator_time: 0.0060 (0.0060)  time: 0.6638  data: 0.0160  max mem: 1699\n",
      "Test:  [49/50]  eta: 0:00:00  model_time: 0.5874 (0.6279)  evaluator_time: 0.0080 (0.0125)  time: 0.6475  data: 0.0287  max mem: 1699\n",
      "Test: Total time: 0:00:33 (0.6745 s / it)\n",
      "Averaged stats: model_time: 0.5874 (0.6279)  evaluator_time: 0.0080 (0.0125)\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.02s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.02s).\n",
      "IoU metric: bbox\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.710\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.984\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.914\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.492\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.720\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.335\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.762\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.762\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.686\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.767\n",
      "IoU metric: segm\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.710\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.986\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.843\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.485\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.720\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.327\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.742\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.742\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.700\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.744\n",
      "Epoch: [1]  [  0/120]  eta: 0:03:38  lr: 0.005000  loss: 0.3763 (0.3763)  loss_classifier: 0.0654 (0.0654)  loss_box_reg: 0.1210 (0.1210)  loss_mask: 0.1823 (0.1823)  loss_objectness: 0.0005 (0.0005)  loss_rpn_box_reg: 0.0072 (0.0072)  time: 1.8201  data: 0.0568  max mem: 1699\n",
      "Epoch: [1]  [ 10/120]  eta: 0:02:55  lr: 0.005000  loss: 0.3703 (0.4323)  loss_classifier: 0.0573 (0.0693)  loss_box_reg: 0.1282 (0.1474)  loss_mask: 0.1837 (0.2002)  loss_objectness: 0.0011 (0.0050)  loss_rpn_box_reg: 0.0079 (0.0103)  time: 1.5913  data: 0.0384  max mem: 1699\n",
      "Epoch: [1]  [ 20/120]  eta: 0:02:34  lr: 0.005000  loss: 0.3254 (0.3673)  loss_classifier: 0.0482 (0.0550)  loss_box_reg: 0.0874 (0.1251)  loss_mask: 0.1606 (0.1730)  loss_objectness: 0.0015 (0.0057)  loss_rpn_box_reg: 0.0068 (0.0085)  time: 1.5324  data: 0.0335  max mem: 1699\n",
      "Epoch: [1]  [ 30/120]  eta: 0:02:20  lr: 0.005000  loss: 0.3110 (0.3808)  loss_classifier: 0.0387 (0.0581)  loss_box_reg: 0.0929 (0.1306)  loss_mask: 0.1568 (0.1777)  loss_objectness: 0.0014 (0.0046)  loss_rpn_box_reg: 0.0051 (0.0098)  time: 1.5416  data: 0.0400  max mem: 1699\n",
      "Epoch: [1]  [ 40/120]  eta: 0:02:02  lr: 0.005000  loss: 0.2600 (0.3476)  loss_classifier: 0.0462 (0.0550)  loss_box_reg: 0.0635 (0.1150)  loss_mask: 0.1328 (0.1649)  loss_objectness: 0.0013 (0.0042)  loss_rpn_box_reg: 0.0049 (0.0084)  time: 1.5162  data: 0.0451  max mem: 1699\n",
      "Epoch: [1]  [ 50/120]  eta: 0:01:46  lr: 0.005000  loss: 0.2482 (0.3377)  loss_classifier: 0.0378 (0.0535)  loss_box_reg: 0.0635 (0.1088)  loss_mask: 0.1295 (0.1616)  loss_objectness: 0.0012 (0.0041)  loss_rpn_box_reg: 0.0037 (0.0098)  time: 1.4739  data: 0.0333  max mem: 1699\n",
      "Epoch: [1]  [ 60/120]  eta: 0:01:30  lr: 0.005000  loss: 0.2642 (0.3303)  loss_classifier: 0.0355 (0.0504)  loss_box_reg: 0.0668 (0.1046)  loss_mask: 0.1397 (0.1623)  loss_objectness: 0.0011 (0.0037)  loss_rpn_box_reg: 0.0056 (0.0093)  time: 1.4808  data: 0.0284  max mem: 1699\n",
      "Epoch: [1]  [ 70/120]  eta: 0:01:15  lr: 0.005000  loss: 0.2383 (0.3176)  loss_classifier: 0.0286 (0.0476)  loss_box_reg: 0.0654 (0.1004)  loss_mask: 0.1292 (0.1575)  loss_objectness: 0.0007 (0.0033)  loss_rpn_box_reg: 0.0056 (0.0088)  time: 1.4935  data: 0.0318  max mem: 1699\n",
      "Epoch: [1]  [ 80/120]  eta: 0:01:01  lr: 0.005000  loss: 0.2225 (0.3134)  loss_classifier: 0.0286 (0.0466)  loss_box_reg: 0.0521 (0.0978)  loss_mask: 0.1292 (0.1573)  loss_objectness: 0.0006 (0.0032)  loss_rpn_box_reg: 0.0053 (0.0084)  time: 1.5586  data: 0.0357  max mem: 1699\n",
      "Epoch: [1]  [ 90/120]  eta: 0:00:45  lr: 0.005000  loss: 0.2304 (0.3073)  loss_classifier: 0.0333 (0.0454)  loss_box_reg: 0.0584 (0.0956)  loss_mask: 0.1409 (0.1549)  loss_objectness: 0.0006 (0.0032)  loss_rpn_box_reg: 0.0052 (0.0081)  time: 1.5086  data: 0.0330  max mem: 1699\n",
      "Epoch: [1]  [100/120]  eta: 0:00:30  lr: 0.005000  loss: 0.2339 (0.3044)  loss_classifier: 0.0339 (0.0444)  loss_box_reg: 0.0584 (0.0933)  loss_mask: 0.1407 (0.1556)  loss_objectness: 0.0009 (0.0031)  loss_rpn_box_reg: 0.0040 (0.0079)  time: 1.4388  data: 0.0280  max mem: 1699\n",
      "Epoch: [1]  [110/120]  eta: 0:00:15  lr: 0.005000  loss: 0.2531 (0.3017)  loss_classifier: 0.0294 (0.0433)  loss_box_reg: 0.0495 (0.0912)  loss_mask: 0.1386 (0.1565)  loss_objectness: 0.0007 (0.0029)  loss_rpn_box_reg: 0.0024 (0.0076)  time: 1.4541  data: 0.0315  max mem: 1699\n",
      "Epoch: [1]  [119/120]  eta: 0:00:01  lr: 0.005000  loss: 0.2531 (0.3024)  loss_classifier: 0.0286 (0.0430)  loss_box_reg: 0.0624 (0.0906)  loss_mask: 0.1386 (0.1583)  loss_objectness: 0.0006 (0.0029)  loss_rpn_box_reg: 0.0032 (0.0076)  time: 1.4449  data: 0.0326  max mem: 1699\n",
      "Epoch: [1] Total time: 0:02:59 (1.4993 s / it)\n",
      "creating index...\n",
      "index created!\n",
      "Test:  [ 0/50]  eta: 0:00:33  model_time: 0.6383 (0.6383)  evaluator_time: 0.0120 (0.0120)  time: 0.6662  data: 0.0150  max mem: 1699\n",
      "Test:  [49/50]  eta: 0:00:00  model_time: 0.6064 (0.6381)  evaluator_time: 0.0139 (0.0191)  time: 0.6680  data: 0.0277  max mem: 1699\n",
      "Test: Total time: 0:00:34 (0.6862 s / it)\n",
      "Averaged stats: model_time: 0.6064 (0.6381)  evaluator_time: 0.0139 (0.0191)\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.02s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.02s).\n",
      "IoU metric: bbox\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.672\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.977\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.812\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.596\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.682\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.327\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.746\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.749\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.686\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.753\n",
      "IoU metric: segm\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.642\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.971\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.772\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.377\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.654\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.312\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.701\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.705\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.614\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.711\n",
      "That's it!\n"
     ]
    }
   ],
   "source": [
    "from engine import train_one_epoch, evaluate\n",
    "import utils\n",
    "\n",
    "\n",
    "\n",
    "# train on the GPU or on the CPU, if a GPU is not available\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# our dataset has two classes only - background and person\n",
    "num_classes = 2\n",
    "# use our dataset and defined transformations\n",
    "dataset = PennFudanDataset('../datasets/PennFudanPed', get_transform(train=True))\n",
    "dataset_test = PennFudanDataset('../datasets/PennFudanPed', get_transform(train=False))\n",
    "\n",
    "# split the dataset in train and test set\n",
    "indices = torch.randperm(len(dataset)).tolist()\n",
    "dataset = torch.utils.data.Subset(dataset, indices[:-50])\n",
    "dataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])\n",
    "\n",
    "# define training and validation data loaders\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=1, shuffle=True, num_workers=0,\n",
    "    collate_fn=utils.collate_fn)\n",
    "\n",
    "data_loader_test = torch.utils.data.DataLoader(\n",
    "    dataset_test, batch_size=1, shuffle=False, num_workers=0,\n",
    "    collate_fn=utils.collate_fn)\n",
    "\n",
    "# get the model using our helper function\n",
    "model = get_model_instance_segmentation(num_classes)\n",
    "\n",
    "# move model to the right device\n",
    "model.to(device)\n",
    "\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005,\n",
    "                            momentum=0.9, weight_decay=0.0005)\n",
    "# and a learning rate scheduler\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                               step_size=3,\n",
    "                                               gamma=0.1)\n",
    "\n",
    "# let's train it for 10 epochs\n",
    "num_epochs = 2\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # train for one epoch, printing every 10 iterations\n",
    "    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
    "    # update the learning rate\n",
    "    lr_scheduler.step()\n",
    "    # evaluate on the test dataset\n",
    "    evaluate(model, data_loader_test, device=device)\n",
    "\n",
    "print(\"That's it!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4682866-5ca3-43e8-846b-d98fd02cd5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model,\"model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bb7340e-417b-42b2-a4c2-36f414a9026f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60a8e18b-3241-4734-8feb-40826105fa0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\11426\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'boxes': tensor([[161.2494, 175.6026, 295.8871, 425.7661],\n",
      "        [421.8279, 173.8446, 522.5903, 466.3010]], device='cuda:0',\n",
      "       grad_fn=<StackBackward>), 'labels': tensor([1, 1], device='cuda:0'), 'scores': tensor([0.9905, 0.9895], device='cuda:0', grad_fn=<IndexBackward>), 'masks': tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',\n",
      "       grad_fn=<UnsqueezeBackward0>)}]\n"
     ]
    }
   ],
   "source": [
    "test_T = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor()\n",
    "])\n",
    "model.eval()\n",
    "img = Image.open(\"../datasets/PennFudanPed/PNGImages/FudanPed00001.png\").convert(\"RGB\")\n",
    "img = np.array(img)\n",
    "img = test_T(img)\n",
    "img = img.reshape((1,img.shape[0],img.shape[1],img.shape[2]))\n",
    "img = img.to(\"cuda\")\n",
    "print(model(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa301c8-694b-4de1-8bb3-5725d50c3848",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
